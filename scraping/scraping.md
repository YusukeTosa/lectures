# 目次
1. 基礎知識
2. Pythonによるクローリング・スクレイピング入門
3. 実用
4. クローラーの継続運用

## 目標
pythonで簡単なクローリング・スクレイピングを実装できるようになるのが目標。  
また、発展的な内容も軽く紹介し、応用タスクに直面したときに何をするべきか迷わないようにする。

# １：基礎知識
<dl>
  <dt>・クローリング</dt>
  <dd>Webページのハイパーリンクをたどって次々にWebページをダウンロードする操作。</dd>
  <dt>・スクレイピング</dt>
  <dd>ダウンロードしたWebページから必要な情報を抜き出す操作。</dd>
</dl>  

## Unixコマンドによる例

### クローリング
Webページをダウンロードするためのコマンド。  

    wget <url> [-O <保存するファイル名>]

標準出力
    
    wget <url> -q -O -

再帰的にリンクをたどる

    wget -r --no-parent -w 1 -l 1 <url>

### スクレイピング
grepや正規表現など。割愛。

## 知っておくべき概念 
* urlの構造
* サーバーとクライアントの概念
* リクエストとレスポンス
* Webサイトの構成
* エンコーディング  
* ファイルの種類  
    * html
    * xml
    * zip  
など…  
* 保存形式
    * txt
    * csv, tsv
    * json  
など…


# ２：Pythonによるクローリング・スクレイピング入門
前述のUnixコマンドでもWebページのデータは処理できるが、複雑な処理には向かない。  
Pythonには簡単な記述でスクレイピング・クローリングできるライブラリがいくつもある。  
本資料では、RequestsとBeautifulSoupを用いた基本的なクローリング・スクレイピングの流れを紹介。

    pip3 install requests beautifulsoup4 # requirements.txtでinstall済み

## クローリング・スクレイピングの基本的な流れ
1. クローラーがファイルをダウンロード(クローリング)
2. データを処理(スクレイピング)
3. データを保存

## この章の構成(sample_code.ipynbを参照)
1. Requestsの基礎
2. BeautifulSoupの基礎
3. データをDBに保存
4. クローラーの作成(`test.py`)
<br>

## 補足
スクレイピング後のデータを格納するDBは以下のコマンドで作る。

    create table qiita_trends(
        date date primary key,
        article_id char(20),
        title text,
        author_id varchar(20),
    );

## クローラー作成の際の注意事項
### クローリングの条件
* robots.txtやmetaタグで許可されていないコンテンツはクロールしない。
* 相手のサーバーに負荷をかけないために、クロール速度を制限する。クロール速度の目安は1page/秒以下

### 新規データのみ取得
httpヘッダーで更新確認。
もしヘッダーに以下のような要素があればキャッシュが利用可能か確認できる。
* Last-Modified
* ETag
* Cache-Control
* Pragma
* Expires  

詳細は以下。  
https://developer.mozilla.org/ja/docs/Web/HTTP/Headers  

CacheControlというpythonライブラリで実装可能。(sanple_code.ipynb A.予備資料 を参照)

### データのバリデーション
assert文や、if raise文などを利用して不正なデータをはじく。  
ムダなリソースの削減のため。

### エラー処理
リクエストに失敗した場合は、リトライ間隔を指数関数的に増やすなどの対策をする。

# ３：実用
実際の現場でクローリング・スクレイピングを実行する際には、２章までで説明したような技術を基礎として、それぞれのタスクに応じた形で処理を行う必要がある。その際何をすればいいか迷わないよう、どんなタスクでどのような処理を行えば良いか、メモ書き程度の粒度ではあるが紹介する。

## 自然言語処理
文章の解析を行いたい場合。  
MeCab, janomeなどで形態素解析。

## APIの利用
アプリケーションによってはクローリング・スクレイピングのためのAPIが提供されていることがあるので、可能なら利用した方がいい。
TwitterからはREST API、AmazonからはProduct Advertisiong APIが利用可能。

## javascriptの解釈&ブラウザの自動操作
SPA(Single Page Application)などhtmlエレメントをjavascriptで生成しているWebサイトは今までの方法ではクローリングは困難。  
従って、JavaScriptを解釈するクローラーが必要。  
pythonではseleniumがよく用いられる。
(https://selenium-python.readthedocs.io/getting-started.html)  
また、フォームの入力などによって得られる情報を収集したい場合は、ブラウザドライバーを用いてブラウザの自動操作をすると直感的に操作できて良い。


# ４：クローラーの継続運用
最後に、クローラーの継続運用に当たって留意すべき事項をいくつか挙げる。本資料では非常に簡易的な説明にとどめる。
より詳細な解説は、「Pythonクローリング&スクレイピング 7章」を参照されたい。

## クローラーの定期的な実行
linux上でジョブを定期的に実行するためにcronを用いることができる。ターミナルで以下を実行。

    crontab -e

エディターが立ち上がるので、次の行を追記。

    {m} {h}   * * *   ~/lectures/scraping/run_crawler.sh > /tmp/crawler.log 2>&1

ターミナルに戻り、

    touch run_crawler.sh

ファイルの内容を以下のようにする。

    #!/bin/bash
    cd $(dirname $0)
    . ../.venv/bin/activate
    python3 test.py

これで、毎日{h}時{m}分にtest.pyを自動で実行するようになる。
ログは/tmp/crawler.logに出力される。

## クローリングとスクレイピングの分離
test.pyではクローリングとスクレイピングを一度に行っているが、大規模なクローリングを行う場合はこれらの処理を分離する方が良い。  
分離することで、スクレイピングの処理が失敗してもクローリングからやり直さずに済み、また、クローリングとスクレイピングの処理をそれぞれ独立にスケールしやすくなる。  
具体的には、メッセージキューを使用することで処理の分離が可能になる。  
pythonでは、RQやRedisといったライブラリがよく用いられている。少々発展的な内容になるため、説明は本資料では割愛する。

## 非同期処理による高速化
多数のサイトのソースをダウンロードする場合、処理にかかる全時間の内、I/Oの占める割合が大きくなる。この場合、CPUリソースを有効活用するために、I/Oの処理を並列実行すると処理時間を削減できる。  
pythonでは、asyncioやaiohttpといったライブラリがよく用いられている。こちらも、発展的な内容になるため、説明は割愛する。
