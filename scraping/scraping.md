# 目次
1. 基礎知識
2. Pythonによるクローリング・スクレイピング入門
3. 実用
4. クローラーの継続運用

## 目標
* 1回生は2まで  
    1, 2は基礎知識として抑えて欲しいです。普段の業務にも活きてくると思います。3は軽く読み流す程度でいいです。必要になったら見返してください。

* 2回生は3まで  
    1は基本的な話なので一読すればOKです。2, 3は実用的な話を多く含むので重要度は比較的高いです。4は少し高度な話題になりますので、必要になったらやるくらいでいいと思います。


# １：基礎知識
<dl>
  <dt>・クローリング</dt>
  <dd>Webページのハイパーリンクをたどって次々にWebページをダウンロードする操作。</dd>
  <dt>・スクレイピング</dt>
  <dd>ダウンロードしたWebページから必要な情報を抜き出す操作。</dd>
</dl>  

## Unixコマンドによる例

### クローリング
Webページをダウンロードするためのコマンド。  

    wget <url> [-O <保存するファイル名>]

標準出力
    
    wget <url> -q -O -

再帰的にリンクをたどる

    wget -r --no-parent -w 1 -l 1 <url>

### スクレイピング
grepや正規表現など。割愛。

## 知っておくべき概念 
* urlの構造
* サーバーとクライアントの概念
* リクエストとレスポンス
* Webサイトの構成
* エンコーディング  
* ファイルの種類  
    * html
    * xml
    * zip  
など…  
* 保存形式
    * txt
    * csv, tsv
    * json  
など…


# ２：Pythonによるクローリング・スクレイピング入門
前述のUnixコマンドでもWebページのデータは処理できるが、複雑な処理には向かない。  
Pythonには簡単な記述でスクレイピング・クローリングできるライブラリがいくつもある。  
本資料では、RequestsとBeautifulSoupを用いた基本的なクローリング・スクレイピングの流れを紹介。

    pip install requests beautifulsoup4 # requirements.txtでinstall済み

## クローリング・スクレイピングの基本的な流れ
1. クローラーがファイルをダウンロード(クローリング)
2. データを処理(スクレイピング)
3. データを保存

## この章の構成(sample_code.ipynbを参照)
1. Requestsの基礎
2. BeautifulSoupの基礎
3. データをDBに保存
4. クローラーの作成(`test.py`)
<br>

# ３：実用
実際の現場でクローリング・スクレイピングを実行する際には、２章までで説明したような技術を基礎として、それぞれのタスクに応じた形で処理を行う必要がある。その際何をすればいいか迷わないよう、どんなタスクでどのような処理を行えば良いか、メモ書き程度の粒度ではあるが紹介する。

## クローラー作成の際の注意事項
### クローリングの条件
* robots.txtやmetaタグで許可されていないコンテンツはクロールしない。
* 相手のサーバーに負荷をかけないために、同時接続数とクロール間隔を制限する。

### 新規データのみ取得
httpヘッダーで更新確認

### データのバリデーション
assert文や、if raise文など。  
ムダなリソースの削減。

## 自然言語処理
文章の解析を行いたい場合。  
MeCabなどで形態素解析。

## APIの利用
Twitterからデータ収集。  
Amazonからデータ収集。

## javascriptの解釈&ブラウザの自動操作
SPAなどは通常の方法ではクローリングは不可能。  
従って、JavaScriptを解釈するクローラーが必要。  
また、フォームの入力などによって得られる情報を収集したい場合は、ブラウザドライバーを用いてブラウザの自動操作をすると直感的に操作できて良い。


# ４：クローラーの継続運用
## クローラーの定期的な実行
cron

## クローリングとスクレイピングの分離
メッセージキュー(rq, redis)

## 非同期処理による高速化
asyncioとaiohttp
